---
title: "STA545 Project"
author: |
    | Estimation of Obesity levels
    | Team: 16
date: "`r Sys.Date()`"
output: pdf_document
extra_dependencies: ['amsmath', 'someotherpackage']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
```

Importing data

```{r}
# importing dataset - csv file

data <- read.csv("ObesityData.csv")
data
summary(data)
```
# Exploratory Data Analysis

```{r}
# Checking for null values
any(is.na(data))
```

Dataset doesn't contain any null values.

```{r}
# Numeric predictors
num.cols <- c("Age", "Height", "Weight")

# Categorical Predictors
cat.cols <- c("Gender", "family_history_with_overweight", "FAVC", "FCVC",
              "NCP", "CAEC", "SMOKE", "CH2O", "SCC", "FAF", "TUE", "CALC",
              "MTRANS")
```


Rounding categorical variables like  'NCP','FCVC','CH20','FAF','TUT'

```{r}
data$FCVC <- round(data$FCVC) # Round off the column to integer
data$NCP <- round(data$NCP) # Round off the column to integer
data$CH2O <- round(data$CH2O) # Round off the column to integer
data$FAF <- round(data$FAF) # Round off the column to integer
data$TUE <- round(data$TUE) # Round off the column to integer

#unique(data[("FCVC")])
```




```{r}
# Analyzing the target variable
library(tidyverse)

obesity.level <- data$NObeyesdad

ggplot(data = data, aes(x=obesity.level)) + geom_bar(stat='count') + 
  stat_count(geom = "text", colour = "white", size = 3.5, aes(label = ..count..), position=position_stack(vjust=0.5))
```
```{r}

# Total count of each category in target variable

data %>% count(NObeyesdad)
```
The above bar graph and the distribution of the data shows that Obesity_Type_I is the most common among the respondents and Insufficient_Weight is the least common one



- Analyzing the distribution of the numerical variables
```{r}
for (var in num.cols) {
  #col <- eval(as.name(paste(var)))
  print(ggplot(data, aes(x=eval(as.name(paste(var))),y=after_stat(density))) + xlab(var) +
  geom_histogram(position='dodge', binwidth=1, fill="#FF9999", color="#e9ecef") +
  geom_density(alpha=0.25))
}

```
Analyzing the categorical data and count

```{r}
# Categorical predictors count

library(dplyr) 
library(plyr)

for (var in cat.cols) {
  data %>% count(data[[var]]) %>% print()
}

```
```{r}

counts <- table(data$family_history_with_overweight)
barplot(counts, main="Number of Respondents with Family History of Overweightness",
xlab="family_history_of_overweightness",col=c("blue","red"))

counts_1 <- table(data$FAVC)
barplot(counts, main="Number of Respondents that Frequently Consume High Caloric Food",
xlab="High-Calorie Food Consumption?",ylab = "Number of Respondents", col=c("blue","red"))

counts_2 <- table(data$SCC)
barplot(counts, main="Number of Respondents that Monitor Calorie Consumption",
xlab="Calorie Consumption Monitoring?",ylab = "Number of Respondents", col=c("blue","red"))

counts_3 <- table(data$SMOKE)
barplot(counts, main="Number of Respondents that Smoke",
xlab="Smokes?",ylab = "Number of Respondents", col=c("blue","red"))


```
Relationship between weight & height specifically , since they are used in calculating BMI

Filtering the data based on genders

```{r}
# Male data
library(easyreg)
data1 = data.frame(filter(data,Gender == 'Male'))
#data %>% filter()
data1=data.frame(data1$Weight,data1$Height)
regplot(data1,model=1,digits=3, position=3, ylab="height", xlab="weight",col="red",
        main = "Relationship between Weight & Height for Females")


```

```{r}


```




```{r}
# Female data
library(easyreg)
library(tidyverse)
library(dplyr)
data2=data.frame(filter(data,Gender == "Female"))
#data %>% filter()
data2=data2[c("Weight","Height")]
regplot(data2,model=1,digits=3, position=3, ylab="height", xlab="weight",col="red",
        main = "Relationship between Weight & Height for Females")

```
This graph shows us there is a trending upwards relationship between weight and height with both genders, with the regression line for females slightly steeper than that of males, meaning that the same increase in weight for females corresponds to a slightly larger increase in height. We can also see that data points corresponding to weights of male are more clustered than females.


### CALC
```{r}
library(plyr)
# Encoding categorical to numeric
dat <- cbind(data)

# Label encoding categorical predictors with two levels into binary 
dat$Gender <- ifelse(dat$Gender == "Male", 1, 0)
dat$FAVC <- ifelse(dat$FAVC == "yes", 1, 0)
dat$SMOKE <- ifelse(dat$SMOKE == "yes", 1, 0)
dat$SCC <- ifelse(dat$SCC == "yes", 1, 0)
#dat$CALC <- ifelse(dat$CALC == "yes", 1, 0)
dat$CALC <- mapvalues(dat$CALC, 
          from=c("Always", "Frequently","Sometimes","no"), 
          to=c(4,3,2,1))

dat$family_history_with_overweight <- ifelse(dat$family_history_with_overweight == "yes", 1, 0)

```

```{r}
# One hot encoding categorical predictors with more than two levels

library(caret)

one.hot <- dummyVars(~ CAEC + MTRANS, data = dat, fullRank = T)
dat_encoded <- data.frame(predict(one.hot, newdata = dat))

```

```{r}
# Replacing categorical values in response column with numeric values

library(plyr)

dat$NObeyesdad <- mapvalues(dat$NObeyesdad, 
          from=c("Insufficient_Weight","Normal_Weight","Obesity_Type_I", "Obesity_Type_II",
                 "Obesity_Type_III", "Overweight_Level_I", "Overweight_Level_II"), 
          to=c(0, 1, 2, 3, 4, 5, 6))
```


```{r}

# merging data frame

data.final <- cbind(dat, dat_encoded)
data.final <- select(data.final, -CAEC, -MTRANS)
```


```{r}
# Analyzing the correlation of variables
library(PerformanceAnalytics)

# converting datatype to numeric
df <- sapply(data.final, as.numeric)

chart.Correlation(df[,c('Age', 'Height', 'Weight', 'NObeyesdad')], histogram=TRUE, pch=19)
```


# Modelling

```{r}
# train test split
library(caret)

set.seed(545)
# stratified split; train: 75%, test: 25%
indices <- createDataPartition(data.final$NObeyesdad, p = 0.75, list = FALSE)

train <- data.final[indices,]
test <- data.final[-indices,]
```

### Decision Tree

```{r}
# Decision Tree
library(rpart)
library(rpart.plot)

tree.fit <- rpart(NObeyesdad ~ . , data = train)
summary(tree.fit)
```


```{r}
rpart.plot(tree.fit, trace=-1)
```
### Decision tree test

```{r}
# Train Test set prediction result

tree.predtrain <- predict(tree.fit, train, type = "class")
tree.predtest <- predict(tree.fit, test, type = "class")

train.error <- mean(tree.predtrain != train$NObeyesdad)
test.error <- mean(tree.predtest != test$NObeyesdad)

print(paste("Misclassification error rate in train = ", train.error))
print(paste("Misclassification error rate in test = ", test.error))
```

```{r}

# Adding new feature BMI (weight/(height^2))

train <- transform(train, BMI=Weight/(Height^2))
test <- transform(test, BMI=Weight/(Height^2))

```

```{r}

df <- sapply(train, as.numeric)

chart.Correlation(df[,c('Age', 'Height', 'Weight', 'BMI', 'NObeyesdad')], 
                  histogram=TRUE, pch=19)
```
BMI can clearly separate the 7 categories of the response variable.

```{r}

tree.fit <- rpart(NObeyesdad ~ . , data = train)

tree.predtrain <- predict(tree.fit, train, type = "class")
tree.predtest <- predict(tree.fit, test, type = "class")

train.error <- mean(tree.predtrain != train$NObeyesdad)
test.error <- mean(tree.predtest != test$NObeyesdad)

print(paste("Misclassification error rate in train = ", train.error))
print(paste("Misclassification error rate in test = ", test.error))

```
Adding new feature 'BMI' decreased the misclassification error rate in test data from 16.6% to 3.6%.

```{r}
# Confusion matrix
table(tree.predtrain, train$NObeyesdad)
table(tree.predtest, test$NObeyesdad)
```

```{r}

# Only using BMI predictor

tree.fit <- rpart(NObeyesdad ~ BMI , data = train)

tree.predtrain <- predict(tree.fit, train['BMI'], type = "class")
tree.predtest <- predict(tree.fit, test['BMI'], type = "class")

train.error <- mean(tree.predtrain != train$NObeyesdad)
test.error <- mean(tree.predtest != test$NObeyesdad)

print(paste("Misclassification error rate in train = ", train.error))
print(paste("Misclassification error rate in test = ", test.error))
```
Only with BMI predictor, the model performs pretty good.


# SVM Modeling

```{r}
library(e1071)

train$CALC <- as.numeric(as.character(train$CALC))

 svm1 <- svm(NObeyesdad~., data=train, 
          type="C-classification", kernal="radial", 
          gamma=0.1, cost=5)
 
 summary(svm1)
```



```{r}
#training prediction
svm.predtrain <- predict(svm1, train)
xtab.train <- table(train$NObeyesdad, svm.predtrain)
print("Confussion matrix for train data")
xtab.train

#test prediction
test$CALC <- as.numeric(as.character(test$CALC))
#test1 <- subset(test, select=-c(CALC))
#df[, colnames(df)[colnames(df) != 'assists']]
svm.predtest <- predict(svm1,test)
xtab.test <- table(test$NObeyesdad, svm.predtest)
print("Confusion matrix for test data")
xtab.test
```
```{r}
svm.train.error <- mean(svm.predtrain != train$NObeyesdad)
svm.test.error <- mean(svm.predtest != test$NObeyesdad)

print(paste("Misclassification error rate in train = ", round(svm.train.error,4),"%"))
print(paste("Misclassification error rate in test = ", round(svm.test.error,4),"%"))

```
```{r}
plot(svm1,test,Age~Height)
```
#Tuning SVM
```{r}
train$NObeyesdad <- as.factor(train$NObeyesdad)
tune.svm <- tune(svm, NObeyesdad~., data=train, kernel="radial",type="C-classification",ranges=list(cost=2^(-3:2),gamma=2^(-25:1)),
              tunecontrol = tune.control(nrepeat = 5, sampling = "cross", cross = 5))
```
#Best model parameters
```{r}
print(paste("Model Parameters: best cost value:", tune.svm$best.parameters[1]))
print(paste("Model Parameters: best gamma value:", tune.svm$best.parameters[2]))

```
#Training on best params
```{r}
svm.bestparam <- svm(NObeyesdad~., 
                     data=train, 
                     type="C-classification", 
                     kernal="radial", 
                     gamma=tune.svm$best.parameters[2],
                     cost=as.numeric(tune.svm$best.parameters[1])
                     )
 
 summary(svm.bestparam)
```
```{r}
#training prediction
svm.bestparam.predtrain <- predict(svm.bestparam, train)
xtab.train <- table(train$NObeyesdad, svm.bestparam.predtrain)
print("Confussion matrix for train data")
xtab.train

#test prediction
svm.bestparam.predtest <- predict(svm.bestparam,test)
xtab.test <- table(test$NObeyesdad, svm.bestparam.predtest)
print("Confusion matrix for test data")
xtab.test
```
```{r}
svm.bestparam.train.error <- mean(svm.bestparam.predtrain != train$NObeyesdad)
svm.bestparam.test.error <- mean(svm.bestparam.predtest != test$NObeyesdad)

print(paste("Misclassification error rate in train = ", round(svm.bestparam.train.error,4),"%"))
print(paste("Misclassification error rate in test = ", round(svm.bestparam.test.error,4),"%"))
```


# Bagging and Boosting Models

```{r}
#bagging

options(warn=-1)

library(ipred)
library(randomForest)

train1<-train

train1$NObeyesdad <- mapvalues(train1$NObeyesdad,
                            from=c(0, 1, 2, 3, 4, 5, 6),
          to=c("Insufficient_Weight","Normal_Weight","Obesity_Type_I", "Obesity_Type_II",
                 "Obesity_Type_III", "Overweight_Level_I", "Overweight_Level_II"))

gbag <- bagging(as.factor(NObeyesdad) ~ ., data = train1,coob=T,nbag=100)
print(gbag)


```


```{r}
#training prediction
bag.predtrain <- predict(gbag, train1)
xtab.train <- table(train$NObeyesdad, bag.predtrain)
print("Confussion matrix for train data")
xtab.train

#test prediction
test1<-test
test1$NObeyesdad <- mapvalues(test1$NObeyesdad,
                            from=c(0, 1, 2, 3, 4, 5, 6),
          to=c("Insufficient_Weight","Normal_Weight","Obesity_Type_I", "Obesity_Type_II",
                 "Obesity_Type_III", "Overweight_Level_I", "Overweight_Level_II"))

bag.predtest <- predict(gbag, test1)
xtab.test <- table(test$NObeyesdad, bag.predtest)
print("Confussion matrix for test data")
xtab.test

```

```{r}
bag.train.error <- mean(bag.predtrain != train1$NObeyesdad)
bag.test.error <- mean(bag.predtest != test1$NObeyesdad)

print(paste("Misclassification error rate in train = ", round(bag.train.error,4),"%"))
print(paste("Misclassification error rate in test = ", round(bag.test.error,4),"%"))
```



```{r}
#calculate variable importance
varimp.data <- subset(train1, select=-c(NObeyesdad))
VI <- data.frame(var=names(varimp.data), imp=varImp(gbag))

#sort variable importance descending
VI_plot <- VI[order(VI$Overall, decreasing=TRUE),]

#visualize variable importance with horizontal bar plot
barplot(VI_plot$Overall,
        names.arg=rownames(VI_plot),
        horiz=FALSE,
        col='steelblue',
        xlab='Variable Importance',
        las=2,
        srt=45)
```

```{r}
#Boosting (gbm)

library(gbm)

options(warn=-1)

boost <- gbm(
  NObeyesdad ~ ., 
  data = train, 
  distribution = "multinomial",
  n.trees = 100,
  #interaction.depth = 4
)
print(boost)
```

```{r}
#training prediction
gbm.predtrain <- predict(boost, train)
xtab.train <- table(train$NObeyesdad, gbm.predtrain)
print("Confussion matrix for train data")
xtab.train

#test prediction
gbm.predtest <- predict(boost, test1)
xtab.test <- confusionMatrix(test1$NObeyesdad, gbm.predtest)
print("Confussion matrix for test data")
xtab.test

```

```{r}
gbm.train.error <- mean(gbm.predtrain != train1$NObeyesdad)
gbm.test.error <- mean(gbm.predtest != test1$NObeyesdad)

print(paste("Misclassification error rate in train = ", round(gbm.train.error,4),"%"))
print(paste("Misclassification error rate in test = ", round(gbm.test.error,4),"%"))
```


### Random forest

```{r}
set.seed(1)
bestmtry <-tuneRF(sapply(train, as.numeric),sapply(train$NObeyesdad, as.numeric),improve = 0.01, stepFactor = 1.5, ntree=5000, trace = TRUE, plot = TRUE)
print(bestmtry)
```


```{r}
set.seed(545)
library(randomForest)

rf.fit <- randomForest(
  NObeyesdad ~ ., 
  data = sapply(train, as.numeric),
  importance = TRUE,
  mtry = 15,
  ntree = 5000
)

rf.pred <- predict(rf.fit, train, type='class')
rf.predtest <- predict(rf.fit, test, type='class')

print("--- Training Error - Random Forest ---")
print(mean(round(rf.pred) != train$NObeyesdad))

print("--- Test Error - Random Forest ---")
print(mean(round(rf.predtest) != test$NObeyesdad))

table(rf.pred, train$NObeyesdad)
table(rf.predtest, test$NObeyesdad)


```

```{r}
rf.fit
varImpPlot(rf.fit)
importance(rf.fit)

```


```{r}
svm.error <- mean(svm.predtrain != train$NObeyesdad)
tree.error <- mean(tree.predtrain != train$NObeyesdad)

print(paste("SVM error = ", svm.error))
print(paste("Tree error = ", tree.error))
```

# Ensemble

```{r}
new_df <- data.frame(svm.predtrain, tree.predtrain, train$NObeyesdad)
entree.fit <- rpart(train.NObeyesdad ~ . , data = new_df)

entree.pred <- predict(entree.fit, new_df, type = "class")
entree.error <- mean(entree.pred != new_df$train.NObeyesdad)

print(paste("Ensemble tree error = ", entree.error))
```









